readme
# 회고 
 ## 박진석
   * 처음 잘못된 접근으로 시간을 너무 많이 사용했습니다.  출력이 데이터셋 문장 그대로 나오는 것을 해결할 계획입니다.
 ## 김재이
  * 배운 점:
    * 트랜스포머에서 데이터 표현이 바뀌는 과정을 배웠다. 그 과정 중 어느 단계에서 연구자가 해야 할 전처리와 토크나이징이 있는지를 다시 배웠다.
        * 벡터로 임베딩되는 과정에서 토크나이징 적용 (토큰화 전 충분히 전처리를 거친다.)
        * 임베딩이 끝나면 포지셔널 인코딩이 최종적으로 적용된 벡터가 학습에 사용된다.
    * 대화 내용을 학습시키기 위해 쌍으로 연결짓는 방법
    * 서브워드 토크나이저의 원리. (그런데 이걸 과제 데이터셋으로 학습시키고, 과제 데이터에 적용하는 게 논리적으로 맞나…?)
    * 토크나이저의 문제를 알아보는 과정 중 SentencePiece도 사전학습모델과 별개로 따로 사용할 수 있다는 걸 알았다.
  * 느낀 점:
    * 앞으로 사전학습 모델을 불러올 때 헷갈리지 않도록, 노드 따라 만들어 본 트랜스포머 구조를 다시 보며 토크나이저를 되짚어 보길 잘 한 것 같다. 토크나이저와 어텐션 마스크 등에서 부족한 지점이 잘 드러났다. 더뎠지만 재미있었다. 연구자로서 선택지가 조금 더 넓어졌기를 바래본다.
    * 팀메이트 진석님과 의논하면서 불필요한 고민을 빨리 제거할 수 있었다. 잘못된 접근도 의미가 있었다. 다시 반복하지 않을만큼 이해했다.
  * 의문점:
    * 두 가지 마스킹이 적용되는 코드를 다시 볼 필요가 있다. 어떻게 두 가지 어텐션이 구현되는지
    * 전처리와 토크나이징이 적용되었는데 왜 원문의 문장이 그대로 출력되는 것인가? (에폭이 작다고 해도 그럴 수 있는 건가 많이 돌려볼 계획입니다.)
